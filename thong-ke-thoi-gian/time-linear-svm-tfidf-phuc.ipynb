{"cells":[{"cell_type":"markdown","source":["https://www.kaggle.com/code/vucongtuanduong/time-linear-svm-tfidf-phuc/output"],"metadata":{"id":"CJQelNwZu6Iw"},"id":"CJQelNwZu6Iw"},{"cell_type":"code","execution_count":null,"id":"bcd85775","metadata":{"execution":{"iopub.execute_input":"2025-04-12T01:03:22.723679Z","iopub.status.busy":"2025-04-12T01:03:22.723411Z","iopub.status.idle":"2025-04-12T01:03:26.629950Z","shell.execute_reply":"2025-04-12T01:03:26.629295Z"},"id":"bcd85775","papermill":{"duration":3.911149,"end_time":"2025-04-12T01:03:26.631455","exception":false,"start_time":"2025-04-12T01:03:22.720306","status":"completed"},"tags":[],"outputId":"0e30d109-115a-4076-ce95-f9ec1dfc3e0e"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt_tab to /usr/share/nltk_data...\n","[nltk_data]   Package punkt_tab is already up-to-date!\n"]}],"source":["import numpy as np\n","import pandas as pd\n","import re\n","import nltk\n","import string\n","nltk.download('stopwords')\n","nltk.download('punkt_tab')\n","from nltk.corpus import stopwords\n","from nltk.stem.porter import PorterStemmer\n","from nltk.tokenize import word_tokenize\n","from sklearn.feature_extraction.text import TfidfVectorizer, HashingVectorizer\n","from sklearn.model_selection import train_test_split\n","from bs4 import BeautifulSoup\n","from sklearn.metrics import confusion_matrix,accuracy_score, f1_score, precision_score,recall_score,classification_report\n","import time\n","from sklearn.svm import SVC\n","from google.colab import drive"]},{"cell_type":"code","execution_count":null,"id":"6d77c111","metadata":{"execution":{"iopub.execute_input":"2025-04-12T01:03:26.637493Z","iopub.status.busy":"2025-04-12T01:03:26.637085Z","iopub.status.idle":"2025-04-12T01:03:26.658124Z","shell.execute_reply":"2025-04-12T01:03:26.657257Z"},"id":"6d77c111","papermill":{"duration":0.025047,"end_time":"2025-04-12T01:03:26.659511","exception":false,"start_time":"2025-04-12T01:03:26.634464","status":"completed"},"tags":[]},"outputs":[],"source":["class HardMarginSVM:\n","    \"\"\"\n","    Optimized Hard Margin SVM implementation using gradient descent\n","\n","    Attributes\n","    -------------\n","    eta : float\n","        Learning rate\n","    epoch : int\n","        Number of epochs\n","    random_state : int\n","        Random seed\n","    is_trained : bool\n","        Training completion flag\n","    num_samples : int\n","        Number of training samples\n","    num_features : int\n","        Number of features\n","    w : NDArray[float]\n","        Parameter vector: (num_features, ) ndarray\n","    b : float\n","        Bias parameter\n","    alpha : NDArray[float]\n","        Lagrange multipliers: (num_samples, ) ndarray\n","    \"\"\"\n","    def __init__(self, eta=0.001, epoch=1000, random_state=42):\n","        self.eta = eta\n","        self.epoch = epoch\n","        self.random_state = random_state\n","        self.is_trained = False\n","        self.support_vectors = None\n","\n","    def fit(self, X, y):\n","        \"\"\"\n","        Fit parameter vector to training data\n","\n","        Parameters\n","        --------------\n","        X : NDArray[NDArray[float]]\n","            Training data: (num_samples, num_features) matrix\n","        y : NDArray[float]\n","            Training labels: (num_samples) ndarray\n","        \"\"\"\n","        # Convert sparse matrix to dense if needed\n","        if hasattr(X, \"toarray\"):\n","            X = X.toarray()\n","\n","        self.num_samples = X.shape[0]\n","        self.num_features = X.shape[1]\n","\n","        y_unique = np.unique(y)\n","        if len(y_unique) != 2:\n","            raise ValueError(\"Binary classification requires exactly 2 classes\")\n","\n","        if set(y_unique) == {0, 1}:\n","            y = np.where(y == 0, -1, 1)\n","\n","        self.w = np.zeros(self.num_features)\n","        self.b = 0\n","\n","\n","        rgen = np.random.RandomState(self.random_state)\n","        self.alpha = rgen.uniform(low=0.0, high=0.01, size=self.num_samples)\n","        for i in range(self.epoch):\n","            self._cycle(X, y)\n","\n","        sv_indices = np.where(self.alpha != 0)[0]\n","\n","        self.support_vectors = sv_indices\n","\n","        self.w = np.zeros(self.num_features)\n","        for i in sv_indices:\n","            self.w += self.alpha[i] * y[i] * X[i]\n","\n","        bias_sum = 0\n","        for i in sv_indices:\n","            bias_sum += y[i] - np.dot(self.w, X[i])\n","\n","        self.b = bias_sum / len(sv_indices)\n","\n","        self.is_trained = True\n","        return self\n","\n","    def predict(self, X):\n","        \"\"\"\n","        Return predictions\n","\n","        Parameters\n","        --------------\n","        X : NDArray[NDArray[float]]\n","            Data to classify: (any, num_features) matrix\n","\n","        Returns\n","        ----------\n","        result : NDArray[int]\n","            Classification results 0 or 1: (any, ) ndarray\n","        \"\"\"\n","        if not self.is_trained:\n","            raise Exception('Model not trained yet')\n","\n","        # Convert sparse matrix to dense if needed\n","        if hasattr(X, \"toarray\"):\n","            X = X.toarray()\n","\n","        decision_values = X @ self.w + self.b\n","\n","        result = np.where(decision_values >= 0, 1, 0)\n","        return result\n","\n","    def _cycle(self, X, y):\n","        \"\"\"\n","        One gradient descent cycle\n","\n","        Parameters\n","        --------------\n","        X : NDArray[NDArray[float]]\n","            Training data: (num_samples, num_features) matrix\n","        y : NDArray[float]\n","            Training labels: (num_samples) ndarray\n","        \"\"\"\n","        y = y.reshape([-1, 1])\n","\n","        XXT = X @ X.T\n","        H = (y @ y.T) * XXT\n","\n","        grad = np.ones(self.num_samples) - H @ self.alpha\n","\n","        self.alpha += self.eta * grad\n","\n","        self.alpha = np.clip(self.alpha, 0, None)\n","\n","\"\"\"## Linear Hard Margin SVM cải tiến V1\n","\n","\"\"\"\n","\n","class HardMarginSVMV1:\n","    \"\"\"\n","    Optimized Hard Margin SVM implementation using gradient descent\n","\n","    Attributes\n","    -------------\n","    eta : float\n","        Learning rate\n","    epoch : int\n","        Number of epochs\n","    random_state : int\n","        Random seed\n","    is_trained : bool\n","        Training completion flag\n","    num_samples : int\n","        Number of training samples\n","    num_features : int\n","        Number of features\n","    w : NDArray[float]\n","        Parameter vector: (num_features, ) ndarray\n","    b : float\n","        Bias parameter\n","    alpha : NDArray[float]\n","        Lagrange multipliers: (num_samples, ) ndarray\n","    \"\"\"\n","    def __init__(self, eta=0.001, epoch=1000, random_state=42, convergence_tol=1e-4):\n","        self.eta = eta\n","        self.epoch = epoch\n","        self.random_state = random_state\n","        self.convergence_tol = convergence_tol\n","        self.is_trained = False\n","        self.support_vectors = None\n","\n","    def fit(self, X, y):\n","        \"\"\"\n","        Fit parameter vector to training data\n","\n","        Parameters\n","        --------------\n","        X : NDArray[NDArray[float]]\n","            Training data: (num_samples, num_features) matrix\n","        y : NDArray[float]\n","            Training labels: (num_samples) ndarray\n","        \"\"\"\n","        self.num_samples = X.shape[0]\n","        self.num_features = X.shape[1]\n","\n","        y_unique = np.unique(y)\n","        if len(y_unique) != 2:\n","            raise ValueError(\"Binary classification requires exactly 2 classes\")\n","\n","        if set(y_unique) == {0, 1}:\n","            y = np.where(y == 0, -1, 1)\n","\n","        self.w = np.zeros(self.num_features)\n","        self.b = 0\n","\n","\n","        rgen = np.random.RandomState(self.random_state)\n","        self.alpha = rgen.uniform(low=0.0, high=0.01, size=self.num_samples)\n","        prev_alpha = np.zeros(self.num_samples)\n","        for i in range(self.epoch):\n","            np.copyto(prev_alpha, self.alpha)\n","\n","            self._cycle(X, y)\n","\n","            if i % 10 == 0:\n","                delta = np.linalg.norm(self.alpha - prev_alpha)\n","                if delta < self.convergence_tol:\n","                    break\n","\n","        sv_indices = np.where(self.alpha != 0)[0]\n","\n","        self.support_vectors = sv_indices\n","\n","        self.w = np.zeros(self.num_features)\n","        for i in sv_indices:\n","            self.w += self.alpha[i] * y[i] * X[i]\n","\n","        bias_sum = 0\n","        for i in sv_indices:\n","            bias_sum += y[i] - np.dot(self.w, X[i])\n","\n","        self.b = bias_sum / len(sv_indices)\n","\n","        self.is_trained = True\n","        return self\n","\n","    def predict(self, X):\n","        \"\"\"\n","        Return predictions\n","\n","        Parameters\n","        --------------\n","        X : NDArray[NDArray[float]]\n","            Data to classify: (any, num_features) matrix\n","\n","        Returns\n","        ----------\n","        result : NDArray[int]\n","            Classification results 0 or 1: (any, ) ndarray\n","        \"\"\"\n","        if not self.is_trained:\n","            raise Exception('Model not trained yet')\n","\n","        decision_values = X @ self.w + self.b\n","\n","        result = np.where(decision_values >= 0, 1, 0)\n","        return result\n","\n","    def _cycle(self, X, y):\n","        \"\"\"\n","        One gradient descent cycle\n","\n","        Parameters\n","        --------------\n","        X : NDArray[NDArray[float]]\n","            Training data: (num_samples, num_features) matrix\n","        y : NDArray[float]\n","            Training labels: (num_samples) ndarray\n","        \"\"\"\n","        y = y.reshape([-1, 1])\n","\n","        XXT = X @ X.T\n","        H = (y @ y.T) * XXT\n","\n","        grad = np.ones(self.num_samples) - H @ self.alpha\n","\n","        self.alpha += self.eta * grad\n","\n","        self.alpha = np.clip(self.alpha, 0, None)\n","\n","\n","\"\"\"## Linear Soft Margin SVM\"\"\"\n","\n","import numpy as np\n","\n","class LinearSVM:\n","    def __init__(self, C=1.0, max_iter=1000, lr=0.001, tolerance=1e-5):\n","        self.C = C\n","        self.max_iter = max_iter\n","        self.lr = lr\n","        self.tolerance = tolerance\n","        self.w = None\n","        self.b = 0\n","\n","    def fit(self, X, y):\n","        # Convert labels to -1, 1 if they're 0, 1\n","        y_binary = np.where(y <= 0, -1, 1)\n","\n","        n_samples, n_features = X.shape\n","\n","        self.w = np.zeros(n_features)\n","\n","        alpha = np.zeros(n_samples)\n","\n","        # Pre-compute Gram matrix to avoid recalculation in the loop\n","        # K[i,j] = y_i * y_j * (x_i · x_j)\n","        K = np.dot(X, X.T) * np.outer(y_binary, y_binary)\n","\n","        # SGD optimization\n","        for iteration in range(self.max_iter):\n","            alpha_prev = alpha.copy()\n","\n","            # Vectorized margin calculation\n","            margins = 1 - K.dot(alpha)\n","\n","            # Update all alphas in one step\n","            mask = margins > 0\n","            alpha[mask] += self.lr * margins[mask]\n","\n","            # Apply box constraint\n","            alpha = np.clip(alpha, 0, self.C)\n","\n","            # Check convergence\n","            if np.max(np.abs(alpha - alpha_prev)) < self.tolerance:\n","                break\n","\n","        # Calculate weights\n","        self.w = np.dot(X.T, alpha * y_binary)\n","\n","        # Calculate bias using support vectors\n","        sv_indices = alpha > 1e-5\n","        if np.any(sv_indices):\n","            self.b = np.mean(y_binary[sv_indices] - np.dot(X[sv_indices], self.w))\n","\n","    def predict(self, X):\n","        \"\"\"Predict class labels for samples in X.\"\"\"\n","        return np.where(np.dot(X, self.w) + self.b >= 0, 1, 0)\n","\n","    def get_parameters(self):\n","      print(f'w: {self.w}')\n","      print(f'b: {self.b}')\n","\n","    def decision_function(self, X):\n","        \"\"\"Return distance of samples to the decision boundary.\"\"\"\n","        return np.dot(X, self.w) + self.b"]},{"cell_type":"code","execution_count":null,"id":"bf1c383f","metadata":{"execution":{"iopub.execute_input":"2025-04-12T01:03:26.663519Z","iopub.status.busy":"2025-04-12T01:03:26.663326Z","iopub.status.idle":"2025-04-12T01:03:26.667703Z","shell.execute_reply":"2025-04-12T01:03:26.666729Z"},"id":"bf1c383f","papermill":{"duration":0.007555,"end_time":"2025-04-12T01:03:26.668875","exception":false,"start_time":"2025-04-12T01:03:26.661320","status":"completed"},"tags":[],"outputId":"b1dda12b-0f8d-4d95-ee6a-1a2e13d8ebe0"},"outputs":[{"name":"stdout","output_type":"stream","text":["[1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500, 5000, 5500, 6000, 6500, 7000, 7500, 8000, 8500, 9000, 9500]\n"]}],"source":["a = [i for i in range(1000, 10000, 500)]\n","print(a)"]},{"cell_type":"code","execution_count":null,"id":"13e5b82c","metadata":{"execution":{"iopub.execute_input":"2025-04-12T01:03:26.673433Z","iopub.status.busy":"2025-04-12T01:03:26.673155Z","iopub.status.idle":"2025-04-12T01:03:26.683407Z","shell.execute_reply":"2025-04-12T01:03:26.682696Z"},"id":"13e5b82c","papermill":{"duration":0.013754,"end_time":"2025-04-12T01:03:26.684484","exception":false,"start_time":"2025-04-12T01:03:26.670730","status":"completed"},"tags":[]},"outputs":[],"source":["def calc_time_svm():\n","  results = []\n","  for i in a:\n","    df = pd.read_csv(f\"https://media.githubusercontent.com/media/PTIT-Projects/ttcs-svm-spam-email/refs/heads/main/dataset/sampled_dataset{i}.csv\")\n","\n","    # viet thuong\n","    df['text'] = df['text'].str.lower()\n","\n","    # xoa ky tu khong phai ASCII\n","    df['text'] = df['text'].apply(lambda x: re.sub(r'[^\\x00-\\x7F]+', '', x) if isinstance(x, str) else x)\n","\n","    # xoa khoang trang\n","    df['text'] = df['text'].apply(lambda x: re.sub(r'^\\s+|\\s+$', '', x).strip() if isinstance(x, str) else x)\n","\n","    # xoa html, xml\n","    def remove_html_xml(text):\n","        try:\n","            soup = BeautifulSoup(text, 'html.parser')\n","            return soup.get_text()\n","        except:\n","            return text\n","\n","    df['text'] = df['text'].apply(remove_html_xml)\n","\n","    # xoa ky tu dac biet\n","    def remove_special_characters(word):\n","        return word.translate(str.maketrans('', '', string.punctuation))\n","\n","    df['text'] = df['text'].apply(remove_special_characters)\n","\n","    # xoa url\n","    def remove_urls(text):\n","        return re.sub(r'http\\S+|www\\S+|\\S+\\.(com|net|org|edu|gov|mil|int|info|biz|co)\\S+', '', text)\n","\n","    df['text'] = df['text'].apply(remove_urls)\n","\n","    # xoa dia chi email\n","    def remove_emails(text):\n","        return re.sub(r'\\S+@\\S+', '', text)\n","\n","    df['text'] = df['text'].apply(remove_emails)\n","\n","    # tach thanh cac tu\n","    df['text'] = df['text'].apply(word_tokenize)\n","\n","    # xoa tu dung(tieng anh)\n","    ENGLISH_STOP_WORDS = set(stopwords.words('english'))\n","\n","    def remove_stop_words(words):\n","        return [word for word in words if word not in ENGLISH_STOP_WORDS]\n","\n","    df['text'] = df['text'].apply(remove_stop_words)\n","\n","    # cat goc tu\n","    stemmer = PorterStemmer()\n","\n","    def stem_words(words):\n","        return [stemmer.stem(word) for word in words]\n","\n","    df['text'] = df['text'].apply(stem_words)\n","\n","    # noi cac tu thanh chuoi\n","    df['text'] = df['text'].apply(' '.join)\n","\n","    # dataset trainning voi test\n","    X = df['text']\n","    y = df['label']\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n","\n","    # tfidf\n","    vectorizer = TfidfVectorizer()\n","    X_train_tfidf = vectorizer.fit_transform(X_train)\n","    X_test_tfidf = vectorizer.transform(X_test)\n","\n","    # # hashing_vectorizer\n","    # hashing_vectorizer = HashingVectorizer(n_features=5000)\n","    # X_train_hashed = hashing_vectorizer.fit_transform(X_train)\n","    # X_test_hashed = hashing_vectorizer.transform(X_test)\n","    X_train_dense = X_train_tfidf.toarray()\n","    X_test_dense = X_test_tfidf.toarray()\n","    result = []\n","    svm_base = LinearSVM()\n","    start_time = time.time()\n","    svm_base.fit(X_train_dense, y_train)\n","    end_time = time.time()\n","    y_pred = svm_base.predict(X_test_dense)\n","    accuracy = accuracy_score(y_test, y_pred)\n","    f1 = f1_score(y_test, y_pred)\n","    result.append({\n","            'class_name': svm_base.__class__.__name__,\n","            'time': end_time - start_time,\n","            'accuracy_score': accuracy,\n","            'f1_score': f1\n","        })\n","    results_df = pd.DataFrame(result)\n","    print(results_df)\n","    results_df.to_csv(f'linear_svm_tfidf_phuc_{i}.csv', index=False)\n","  return results"]},{"cell_type":"code","execution_count":null,"id":"779ebb76","metadata":{"execution":{"iopub.execute_input":"2025-04-12T01:03:26.689206Z","iopub.status.busy":"2025-04-12T01:03:26.688917Z","iopub.status.idle":"2025-04-12T01:14:23.184576Z","shell.execute_reply":"2025-04-12T01:14:23.183669Z"},"id":"779ebb76","papermill":{"duration":656.499268,"end_time":"2025-04-12T01:14:23.185691","exception":false,"start_time":"2025-04-12T01:03:26.686423","status":"completed"},"tags":[],"outputId":"37fa9dba-bbf8-4a48-ae6f-e347b6cf5a39"},"outputs":[{"name":"stdout","output_type":"stream","text":["  class_name     time  accuracy_score  f1_score\n","0  LinearSVM  0.36652            0.94  0.942308\n","  class_name      time  accuracy_score  f1_score\n","0  LinearSVM  0.779361        0.966667  0.969697\n","  class_name      time  accuracy_score  f1_score\n","0  LinearSVM  1.510797           0.965  0.966346\n","  class_name      time  accuracy_score  f1_score\n","0  LinearSVM  2.610204           0.978  0.978474\n","  class_name      time  accuracy_score  f1_score\n","0  LinearSVM  4.486905        0.966667  0.967105\n","  class_name      time  accuracy_score  f1_score\n","0  LinearSVM  6.691292        0.964286  0.967658\n","  class_name      time  accuracy_score  f1_score\n","0  LinearSVM  8.811273         0.95875  0.962963\n","  class_name       time  accuracy_score  f1_score\n","0  LinearSVM  11.114721        0.963333  0.965227\n","  class_name       time  accuracy_score  f1_score\n","0  LinearSVM  13.884588           0.974  0.975746\n","  class_name       time  accuracy_score  f1_score\n","0  LinearSVM  17.598646        0.969091  0.971039\n","  class_name       time  accuracy_score  f1_score\n","0  LinearSVM  21.216293        0.976667  0.978091\n","  class_name       time  accuracy_score  f1_score\n","0  LinearSVM  25.597174        0.977692  0.978723\n","  class_name       time  accuracy_score  f1_score\n","0  LinearSVM  31.035673           0.975  0.976526\n","  class_name       time  accuracy_score  f1_score\n","0  LinearSVM  36.023438        0.973333  0.974194\n","  class_name       time  accuracy_score  f1_score\n","0  LinearSVM  43.586541        0.976875  0.977831\n","  class_name       time  accuracy_score  f1_score\n","0  LinearSVM  48.997304        0.975294  0.976109\n","  class_name       time  accuracy_score  f1_score\n","0  LinearSVM  55.810158        0.977778  0.978701\n","  class_name       time  accuracy_score  f1_score\n","0  LinearSVM  63.341173        0.974211  0.975561\n"]},{"data":{"text/plain":["[]"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["calc_time_svm()"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"papermill":{"default_parameters":{},"duration":664.823049,"end_time":"2025-04-12T01:14:23.908292","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-04-12T01:03:19.085243","version":"2.6.0"}},"nbformat":4,"nbformat_minor":5}