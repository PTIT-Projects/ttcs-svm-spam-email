{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMGF98LMRwqrsdzztxOltmP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import re\n","import nltk\n","import string\n","nltk.download('stopwords')\n","nltk.download('punkt_tab')\n","from nltk.corpus import stopwords\n","from nltk.stem.porter import PorterStemmer\n","from nltk.tokenize import word_tokenize\n","from sklearn.feature_extraction.text import TfidfVectorizer, HashingVectorizer\n","from sklearn.model_selection import train_test_split\n","from bs4 import BeautifulSoup\n","from sklearn.metrics import confusion_matrix,accuracy_score, f1_score, precision_score,recall_score,classification_report\n","import time\n","from sklearn.svm import SVC\n","from google.colab import drive\n","import pickle"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f0O2Yk_GQXHW","executionInfo":{"status":"ok","timestamp":1744431787748,"user_tz":-420,"elapsed":115,"user":{"displayName":"Duong Vu COng","userId":"01870437980576342211"}},"outputId":"1ca7ef29-d11f-428f-d949-95d643b22302"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Package punkt_tab is already up-to-date!\n"]}]},{"cell_type":"code","source":["class HardMarginSVM:\n","    \"\"\"\n","    Optimized Hard Margin SVM implementation using gradient descent\n","\n","    Attributes\n","    -------------\n","    eta : float\n","        Learning rate\n","    epoch : int\n","        Number of epochs\n","    random_state : int\n","        Random seed\n","    is_trained : bool\n","        Training completion flag\n","    num_samples : int\n","        Number of training samples\n","    num_features : int\n","        Number of features\n","    w : NDArray[float]\n","        Parameter vector: (num_features, ) ndarray\n","    b : float\n","        Bias parameter\n","    alpha : NDArray[float]\n","        Lagrange multipliers: (num_samples, ) ndarray\n","    \"\"\"\n","    def __init__(self, eta=0.001, epoch=1000, random_state=42):\n","        self.eta = eta\n","        self.epoch = epoch\n","        self.random_state = random_state\n","        self.is_trained = False\n","        self.support_vectors = None\n","\n","    def fit(self, X, y):\n","        \"\"\"\n","        Fit parameter vector to training data\n","\n","        Parameters\n","        --------------\n","        X : NDArray[NDArray[float]]\n","            Training data: (num_samples, num_features) matrix\n","        y : NDArray[float]\n","            Training labels: (num_samples) ndarray\n","        \"\"\"\n","        # Convert sparse matrix to dense if needed\n","        if hasattr(X, \"toarray\"):\n","            X = X.toarray()\n","\n","        self.num_samples = X.shape[0]\n","        self.num_features = X.shape[1]\n","\n","        y_unique = np.unique(y)\n","        if len(y_unique) != 2:\n","            raise ValueError(\"Binary classification requires exactly 2 classes\")\n","\n","        if set(y_unique) == {0, 1}:\n","            y = np.where(y == 0, -1, 1)\n","\n","        self.w = np.zeros(self.num_features)\n","        self.b = 0\n","\n","\n","        rgen = np.random.RandomState(self.random_state)\n","        self.alpha = rgen.uniform(low=0.0, high=0.01, size=self.num_samples)\n","        for i in range(self.epoch):\n","            self._cycle(X, y)\n","\n","        sv_indices = np.where(self.alpha != 0)[0]\n","\n","        self.support_vectors = sv_indices\n","\n","        self.w = np.zeros(self.num_features)\n","        for i in sv_indices:\n","            self.w += self.alpha[i] * y[i] * X[i]\n","\n","        bias_sum = 0\n","        for i in sv_indices:\n","            bias_sum += y[i] - np.dot(self.w, X[i])\n","\n","        self.b = bias_sum / len(sv_indices)\n","\n","        self.is_trained = True\n","        return self\n","\n","    def predict(self, X):\n","        \"\"\"\n","        Return predictions\n","\n","        Parameters\n","        --------------\n","        X : NDArray[NDArray[float]]\n","            Data to classify: (any, num_features) matrix\n","\n","        Returns\n","        ----------\n","        result : NDArray[int]\n","            Classification results 0 or 1: (any, ) ndarray\n","        \"\"\"\n","        if not self.is_trained:\n","            raise Exception('Model not trained yet')\n","\n","        # Convert sparse matrix to dense if needed\n","        if hasattr(X, \"toarray\"):\n","            X = X.toarray()\n","\n","        decision_values = X @ self.w + self.b\n","\n","        result = np.where(decision_values >= 0, 1, 0)\n","        return result\n","\n","    def _cycle(self, X, y):\n","        \"\"\"\n","        One gradient descent cycle\n","\n","        Parameters\n","        --------------\n","        X : NDArray[NDArray[float]]\n","            Training data: (num_samples, num_features) matrix\n","        y : NDArray[float]\n","            Training labels: (num_samples) ndarray\n","        \"\"\"\n","        y = y.reshape([-1, 1])\n","\n","        XXT = X @ X.T\n","        H = (y @ y.T) * XXT\n","\n","        grad = np.ones(self.num_samples) - H @ self.alpha\n","\n","        self.alpha += self.eta * grad\n","\n","        self.alpha = np.clip(self.alpha, 0, None)\n","\n","\n","class LinearSVM:\n","    def __init__(self, C=1.0, max_iter=1000, lr=0.001, tolerance=1e-5):\n","        self.C = C\n","        self.max_iter = max_iter\n","        self.lr = lr\n","        self.tolerance = tolerance\n","        self.w = None\n","        self.b = 0\n","\n","    def fit(self, X, y):\n","        # Convert labels to -1, 1 if they're 0, 1\n","        y_binary = np.where(y <= 0, -1, 1)\n","\n","        n_samples, n_features = X.shape\n","\n","        self.w = np.zeros(n_features)\n","\n","        alpha = np.zeros(n_samples)\n","\n","        # Pre-compute Gram matrix to avoid recalculation in the loop\n","        # K[i,j] = y_i * y_j * (x_i Â· x_j)\n","        K = np.dot(X, X.T) * np.outer(y_binary, y_binary)\n","\n","        # SGD optimization\n","        for iteration in range(self.max_iter):\n","            alpha_prev = alpha.copy()\n","\n","            # Vectorized margin calculation\n","            margins = 1 - K.dot(alpha)\n","\n","            # Update all alphas in one step\n","            mask = margins > 0\n","            alpha[mask] += self.lr * margins[mask]\n","\n","            # Apply box constraint\n","            alpha = np.clip(alpha, 0, self.C)\n","\n","            # Check convergence\n","            if np.max(np.abs(alpha - alpha_prev)) < self.tolerance:\n","                break\n","\n","        # Calculate weights\n","        self.w = np.dot(X.T, alpha * y_binary)\n","\n","        # Calculate bias using support vectors\n","        sv_indices = alpha > 1e-5\n","        if np.any(sv_indices):\n","            self.b = np.mean(y_binary[sv_indices] - np.dot(X[sv_indices], self.w))\n","\n","    def predict(self, X):\n","        \"\"\"Predict class labels for samples in X.\"\"\"\n","        return np.where(np.dot(X, self.w) + self.b >= 0, 1, 0)\n","\n","    def get_parameters(self):\n","      print(f'w: {self.w}')\n","      print(f'b: {self.b}')\n","\n","    def decision_function(self, X):\n","        \"\"\"Return distance of samples to the decision boundary.\"\"\"\n","        return np.dot(X, self.w) + self.b"],"metadata":{"id":"UZjqEIB2gKyM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["a = [i for i in range(1000, 10000, 500)]\n","print(a)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t3vSn1WbT-6m","executionInfo":{"status":"ok","timestamp":1744431787761,"user_tz":-420,"elapsed":7,"user":{"displayName":"Duong Vu COng","userId":"01870437980576342211"}},"outputId":"682cc2ed-2082-493e-ec37-bdddabe0819c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500, 5000, 5500, 6000, 6500, 7000, 7500, 8000, 8500, 9000, 9500]\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PMFloxHDPMgT"},"outputs":[],"source":["def calc_time_svm():\n","  results = []\n","  for i in a:\n","    df = pd.read_csv(f\"https://media.githubusercontent.com/media/PTIT-Projects/ttcs-svm-spam-email/refs/heads/main/dataset/sampled_dataset{i}.csv\")\n","\n","    # viet thuong\n","    df['text'] = df['text'].str.lower()\n","\n","    # xoa ky tu khong phai ASCII\n","    df['text'] = df['text'].apply(lambda x: re.sub(r'[^\\x00-\\x7F]+', '', x) if isinstance(x, str) else x)\n","\n","    # xoa khoang trang\n","    df['text'] = df['text'].apply(lambda x: re.sub(r'^\\s+|\\s+$', '', x).strip() if isinstance(x, str) else x)\n","\n","    # xoa html, xml\n","    def remove_html_xml(text):\n","        try:\n","            soup = BeautifulSoup(text, 'html.parser')\n","            return soup.get_text()\n","        except:\n","            return text\n","\n","    df['text'] = df['text'].apply(remove_html_xml)\n","\n","    # xoa ky tu dac biet\n","    def remove_special_characters(word):\n","        return word.translate(str.maketrans('', '', string.punctuation))\n","\n","    df['text'] = df['text'].apply(remove_special_characters)\n","\n","    # xoa url\n","    def remove_urls(text):\n","        return re.sub(r'http\\S+|www\\S+|\\S+\\.(com|net|org|edu|gov|mil|int|info|biz|co)\\S+', '', text)\n","\n","    df['text'] = df['text'].apply(remove_urls)\n","\n","    # xoa dia chi email\n","    def remove_emails(text):\n","        return re.sub(r'\\S+@\\S+', '', text)\n","\n","    df['text'] = df['text'].apply(remove_emails)\n","\n","    # tach thanh cac tu\n","    df['text'] = df['text'].apply(word_tokenize)\n","\n","    # xoa tu dung(tieng anh)\n","    ENGLISH_STOP_WORDS = set(stopwords.words('english'))\n","\n","    def remove_stop_words(words):\n","        return [word for word in words if word not in ENGLISH_STOP_WORDS]\n","\n","    df['text'] = df['text'].apply(remove_stop_words)\n","\n","    # cat goc tu\n","    stemmer = PorterStemmer()\n","\n","    def stem_words(words):\n","        return [stemmer.stem(word) for word in words]\n","\n","    df['text'] = df['text'].apply(stem_words)\n","\n","    # noi cac tu thanh chuoi\n","    df['text'] = df['text'].apply(' '.join)\n","\n","    # dataset trainning voi test\n","    X = df['text']\n","    y = df['label']\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n","\n","    # tfidf\n","    # vectorizer = TfidfVectorizer()\n","    # X_train_tfidf = vectorizer.fit_transform(X_train)\n","    # X_test_tfidf = vectorizer.transform(X_test)\n","\n","    # # hashing_vectorizer\n","    hashing_vectorizer = HashingVectorizer(n_features=1000)\n","    X_train_hashed = hashing_vectorizer.fit_transform(X_train)\n","    X_test_hashed = hashing_vectorizer.transform(X_test)\n","    # X_train_dense = X_train_tfidf.toarray()\n","    # X_test_dense = X_test_tfidf.toarray()\n","    result = []\n","    svm_base = SVC(kernel = 'linear')\n","    start_time = time.time()\n","    svm_base.fit(X_train_hashed, y_train)\n","    end_time = time.time()\n","    y_pred = svm_base.predict(X_test_hashed)\n","    accuracy = accuracy_score(y_test, y_pred)\n","    f1 = f1_score(y_test, y_pred)\n","    result.append({\n","            'class_name': svm_base.__class__.__name__,\n","            'time': end_time - start_time,\n","            'accuracy_score': accuracy,\n","            'f1_score': f1\n","        })\n","    results_df = pd.DataFrame(result)\n","    results_df.to_csv(f'base_svm_tfidf_{i}.csv', index=False)\n","    with open('linear_svm.pkl', 'wb') as model_file:\n","        pickle.dump(svm_base, model_file)\n","\n","    # Save the vectorizer\n","    with open('hashing_vectorizer.pkl', 'wb') as vectorizer_file:\n","        pickle.dump(hashing_vectorizer, vectorizer_file)\n","  return results"]},{"cell_type":"code","source":["calc_time_svm()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":339},"id":"9NBzmwyBX__q","executionInfo":{"status":"error","timestamp":1744431797149,"user_tz":-420,"elapsed":9340,"user":{"displayName":"Duong Vu COng","userId":"01870437980576342211"}},"outputId":"aad78360-25d8-450c-c4e0-49849f95f4d5"},"execution_count":null,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-28e70ebdaf3c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcalc_time_svm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-9-ea7d6cdb5942>\u001b[0m in \u001b[0;36mcalc_time_svm\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstemmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstem_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;31m# noi cac tu thanh chuoi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4922\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4923\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4924\u001b[0;31m         ).apply()\n\u001b[0m\u001b[1;32m   4925\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4926\u001b[0m     def _reindex_indexer(\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1426\u001b[0m         \u001b[0;31m# self.func is Callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1427\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1429\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1505\u001b[0m         \u001b[0;31m#  Categorical (GH51645).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1506\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCategoricalDtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1507\u001b[0;31m         mapped = obj._map_values(\n\u001b[0m\u001b[1;32m   1508\u001b[0m             \u001b[0mmapper\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurried\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1509\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/base.py\u001b[0m in \u001b[0;36m_map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0malgorithms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1742\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mna_action\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1743\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1744\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m         return lib.map_infer_mask(\n","\u001b[0;32mlib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n","\u001b[0;32m<ipython-input-9-ea7d6cdb5942>\u001b[0m in \u001b[0;36mstem_words\u001b[0;34m(words)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstem_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstemmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstem_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-9-ea7d6cdb5942>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstem_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstemmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstem_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/stem/porter.py\u001b[0m in \u001b[0;36mstem\u001b[0;34m(self, word, to_lowercase)\u001b[0m\n\u001b[1;32m    672\u001b[0m         \u001b[0mstem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step1b\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0mstem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step1c\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 674\u001b[0;31m         \u001b[0mstem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    675\u001b[0m         \u001b[0mstem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m         \u001b[0mstem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/stem/porter.py\u001b[0m in \u001b[0;36m_step2\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    513\u001b[0m             \u001b[0mrules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"logi\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"log\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_positive_measure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_rule_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrules\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_step3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/stem/porter.py\u001b[0m in \u001b[0;36m_apply_rule_list\u001b[0;34m(self, word, rules)\u001b[0m\n\u001b[1;32m    264\u001b[0m                     \u001b[0;31m# Don't try any further rules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuffix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m                 \u001b[0mstem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_replace_suffix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuffix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcondition\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcondition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":[],"metadata":{"id":"1N1u9MGgYCoH"},"execution_count":null,"outputs":[]}]}