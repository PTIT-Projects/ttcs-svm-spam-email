{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8644d174",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T00:54:32.545229Z",
     "iopub.status.busy": "2025-04-12T00:54:32.544920Z",
     "iopub.status.idle": "2025-04-12T00:54:37.612643Z",
     "shell.execute_reply": "2025-04-12T00:54:37.611700Z"
    },
    "id": "f0O2Yk_GQXHW",
    "papermill": {
     "duration": 5.073505,
     "end_time": "2025-04-12T00:54:37.614375",
     "exception": false,
     "start_time": "2025-04-12T00:54:32.540870",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score, f1_score, precision_score,recall_score,classification_report\n",
    "import time\n",
    "from sklearn.svm import SVC\n",
    "from google.colab import drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d937a6d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T00:54:37.622393Z",
     "iopub.status.busy": "2025-04-12T00:54:37.621424Z",
     "iopub.status.idle": "2025-04-12T00:54:37.649879Z",
     "shell.execute_reply": "2025-04-12T00:54:37.648904Z"
    },
    "id": "UZjqEIB2gKyM",
    "papermill": {
     "duration": 0.033854,
     "end_time": "2025-04-12T00:54:37.651474",
     "exception": false,
     "start_time": "2025-04-12T00:54:37.617620",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class HardMarginSVM:\n",
    "    \"\"\"\n",
    "    Optimized Hard Margin SVM implementation using gradient descent\n",
    "\n",
    "    Attributes\n",
    "    -------------\n",
    "    eta : float\n",
    "        Learning rate\n",
    "    epoch : int\n",
    "        Number of epochs\n",
    "    random_state : int\n",
    "        Random seed\n",
    "    is_trained : bool\n",
    "        Training completion flag\n",
    "    num_samples : int\n",
    "        Number of training samples\n",
    "    num_features : int\n",
    "        Number of features\n",
    "    w : NDArray[float]\n",
    "        Parameter vector: (num_features, ) ndarray\n",
    "    b : float\n",
    "        Bias parameter\n",
    "    alpha : NDArray[float]\n",
    "        Lagrange multipliers: (num_samples, ) ndarray\n",
    "    \"\"\"\n",
    "    def __init__(self, eta=0.001, epoch=1000, random_state=42):\n",
    "        self.eta = eta\n",
    "        self.epoch = epoch\n",
    "        self.random_state = random_state\n",
    "        self.is_trained = False\n",
    "        self.support_vectors = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit parameter vector to training data\n",
    "\n",
    "        Parameters\n",
    "        --------------\n",
    "        X : NDArray[NDArray[float]]\n",
    "            Training data: (num_samples, num_features) matrix\n",
    "        y : NDArray[float]\n",
    "            Training labels: (num_samples) ndarray\n",
    "        \"\"\"\n",
    "        # Convert sparse matrix to dense if needed\n",
    "        if hasattr(X, \"toarray\"):\n",
    "            X = X.toarray()\n",
    "\n",
    "        self.num_samples = X.shape[0]\n",
    "        self.num_features = X.shape[1]\n",
    "\n",
    "        y_unique = np.unique(y)\n",
    "        if len(y_unique) != 2:\n",
    "            raise ValueError(\"Binary classification requires exactly 2 classes\")\n",
    "\n",
    "        if set(y_unique) == {0, 1}:\n",
    "            y = np.where(y == 0, -1, 1)\n",
    "\n",
    "        self.w = np.zeros(self.num_features)\n",
    "        self.b = 0\n",
    "\n",
    "\n",
    "        rgen = np.random.RandomState(self.random_state)\n",
    "        self.alpha = rgen.uniform(low=0.0, high=0.01, size=self.num_samples)\n",
    "        for i in range(self.epoch):\n",
    "            self._cycle(X, y)\n",
    "\n",
    "        sv_indices = np.where(self.alpha != 0)[0]\n",
    "\n",
    "        self.support_vectors = sv_indices\n",
    "\n",
    "        self.w = np.zeros(self.num_features)\n",
    "        for i in sv_indices:\n",
    "            self.w += self.alpha[i] * y[i] * X[i]\n",
    "\n",
    "        bias_sum = 0\n",
    "        for i in sv_indices:\n",
    "            bias_sum += y[i] - np.dot(self.w, X[i])\n",
    "\n",
    "        self.b = bias_sum / len(sv_indices)\n",
    "\n",
    "        self.is_trained = True\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Return predictions\n",
    "\n",
    "        Parameters\n",
    "        --------------\n",
    "        X : NDArray[NDArray[float]]\n",
    "            Data to classify: (any, num_features) matrix\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        result : NDArray[int]\n",
    "            Classification results 0 or 1: (any, ) ndarray\n",
    "        \"\"\"\n",
    "        if not self.is_trained:\n",
    "            raise Exception('Model not trained yet')\n",
    "\n",
    "        # Convert sparse matrix to dense if needed\n",
    "        if hasattr(X, \"toarray\"):\n",
    "            X = X.toarray()\n",
    "\n",
    "        decision_values = X @ self.w + self.b\n",
    "\n",
    "        result = np.where(decision_values >= 0, 1, 0)\n",
    "        return result\n",
    "\n",
    "    def _cycle(self, X, y):\n",
    "        \"\"\"\n",
    "        One gradient descent cycle\n",
    "\n",
    "        Parameters\n",
    "        --------------\n",
    "        X : NDArray[NDArray[float]]\n",
    "            Training data: (num_samples, num_features) matrix\n",
    "        y : NDArray[float]\n",
    "            Training labels: (num_samples) ndarray\n",
    "        \"\"\"\n",
    "        y = y.reshape([-1, 1])\n",
    "\n",
    "        XXT = X @ X.T\n",
    "        H = (y @ y.T) * XXT\n",
    "\n",
    "        grad = np.ones(self.num_samples) - H @ self.alpha\n",
    "\n",
    "        self.alpha += self.eta * grad\n",
    "\n",
    "        self.alpha = np.clip(self.alpha, 0, None)\n",
    "\n",
    "\"\"\"## Linear Hard Margin SVM cải tiến V1\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "class HardMarginSVMV1:\n",
    "    \"\"\"\n",
    "    Optimized Hard Margin SVM implementation using gradient descent\n",
    "\n",
    "    Attributes\n",
    "    -------------\n",
    "    eta : float\n",
    "        Learning rate\n",
    "    epoch : int\n",
    "        Number of epochs\n",
    "    random_state : int\n",
    "        Random seed\n",
    "    is_trained : bool\n",
    "        Training completion flag\n",
    "    num_samples : int\n",
    "        Number of training samples\n",
    "    num_features : int\n",
    "        Number of features\n",
    "    w : NDArray[float]\n",
    "        Parameter vector: (num_features, ) ndarray\n",
    "    b : float\n",
    "        Bias parameter\n",
    "    alpha : NDArray[float]\n",
    "        Lagrange multipliers: (num_samples, ) ndarray\n",
    "    \"\"\"\n",
    "    def __init__(self, eta=0.001, epoch=1000, random_state=42, convergence_tol=1e-4):\n",
    "        self.eta = eta\n",
    "        self.epoch = epoch\n",
    "        self.random_state = random_state\n",
    "        self.convergence_tol = convergence_tol\n",
    "        self.is_trained = False\n",
    "        self.support_vectors = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit parameter vector to training data\n",
    "\n",
    "        Parameters\n",
    "        --------------\n",
    "        X : NDArray[NDArray[float]]\n",
    "            Training data: (num_samples, num_features) matrix\n",
    "        y : NDArray[float]\n",
    "            Training labels: (num_samples) ndarray\n",
    "        \"\"\"\n",
    "        self.num_samples = X.shape[0]\n",
    "        self.num_features = X.shape[1]\n",
    "\n",
    "        y_unique = np.unique(y)\n",
    "        if len(y_unique) != 2:\n",
    "            raise ValueError(\"Binary classification requires exactly 2 classes\")\n",
    "\n",
    "        if set(y_unique) == {0, 1}:\n",
    "            y = np.where(y == 0, -1, 1)\n",
    "\n",
    "        self.w = np.zeros(self.num_features)\n",
    "        self.b = 0\n",
    "\n",
    "\n",
    "        rgen = np.random.RandomState(self.random_state)\n",
    "        self.alpha = rgen.uniform(low=0.0, high=0.01, size=self.num_samples)\n",
    "        prev_alpha = np.zeros(self.num_samples)\n",
    "        for i in range(self.epoch):\n",
    "            np.copyto(prev_alpha, self.alpha)\n",
    "\n",
    "            self._cycle(X, y)\n",
    "\n",
    "            if i % 10 == 0:\n",
    "                delta = np.linalg.norm(self.alpha - prev_alpha)\n",
    "                if delta < self.convergence_tol:\n",
    "                    break\n",
    "\n",
    "        sv_indices = np.where(self.alpha != 0)[0]\n",
    "\n",
    "        self.support_vectors = sv_indices\n",
    "\n",
    "        self.w = np.zeros(self.num_features)\n",
    "        for i in sv_indices:\n",
    "            self.w += self.alpha[i] * y[i] * X[i]\n",
    "\n",
    "        bias_sum = 0\n",
    "        for i in sv_indices:\n",
    "            bias_sum += y[i] - np.dot(self.w, X[i])\n",
    "\n",
    "        self.b = bias_sum / len(sv_indices)\n",
    "\n",
    "        self.is_trained = True\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Return predictions\n",
    "\n",
    "        Parameters\n",
    "        --------------\n",
    "        X : NDArray[NDArray[float]]\n",
    "            Data to classify: (any, num_features) matrix\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        result : NDArray[int]\n",
    "            Classification results 0 or 1: (any, ) ndarray\n",
    "        \"\"\"\n",
    "        if not self.is_trained:\n",
    "            raise Exception('Model not trained yet')\n",
    "\n",
    "        decision_values = X @ self.w + self.b\n",
    "\n",
    "        result = np.where(decision_values >= 0, 1, 0)\n",
    "        return result\n",
    "\n",
    "    def _cycle(self, X, y):\n",
    "        \"\"\"\n",
    "        One gradient descent cycle\n",
    "\n",
    "        Parameters\n",
    "        --------------\n",
    "        X : NDArray[NDArray[float]]\n",
    "            Training data: (num_samples, num_features) matrix\n",
    "        y : NDArray[float]\n",
    "            Training labels: (num_samples) ndarray\n",
    "        \"\"\"\n",
    "        y = y.reshape([-1, 1])\n",
    "\n",
    "        XXT = X @ X.T\n",
    "        H = (y @ y.T) * XXT\n",
    "\n",
    "        grad = np.ones(self.num_samples) - H @ self.alpha\n",
    "\n",
    "        self.alpha += self.eta * grad\n",
    "\n",
    "        self.alpha = np.clip(self.alpha, 0, None)\n",
    "\n",
    "\n",
    "\"\"\"## Linear Soft Margin SVM\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class LinearSVM:\n",
    "    def __init__(self, C=1.0, max_iter=1000, lr=0.001, tolerance=1e-5):\n",
    "        self.C = C\n",
    "        self.max_iter = max_iter\n",
    "        self.lr = lr\n",
    "        self.tolerance = tolerance\n",
    "        self.w = None\n",
    "        self.b = 0\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Convert labels to -1, 1 if they're 0, 1\n",
    "        y_binary = np.where(y <= 0, -1, 1)\n",
    "\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        self.w = np.zeros(n_features)\n",
    "\n",
    "        alpha = np.zeros(n_samples)\n",
    "\n",
    "        # Pre-compute Gram matrix to avoid recalculation in the loop\n",
    "        # K[i,j] = y_i * y_j * (x_i · x_j)\n",
    "        K = np.dot(X, X.T) * np.outer(y_binary, y_binary)\n",
    "\n",
    "        # SGD optimization\n",
    "        for iteration in range(self.max_iter):\n",
    "            alpha_prev = alpha.copy()\n",
    "\n",
    "            # Vectorized margin calculation\n",
    "            margins = 1 - K.dot(alpha)\n",
    "\n",
    "            # Update all alphas in one step\n",
    "            mask = margins > 0\n",
    "            alpha[mask] += self.lr * margins[mask]\n",
    "\n",
    "            # Apply box constraint\n",
    "            alpha = np.clip(alpha, 0, self.C)\n",
    "\n",
    "            # Check convergence\n",
    "            if np.max(np.abs(alpha - alpha_prev)) < self.tolerance:\n",
    "                break\n",
    "\n",
    "        # Calculate weights\n",
    "        self.w = np.dot(X.T, alpha * y_binary)\n",
    "\n",
    "        # Calculate bias using support vectors\n",
    "        sv_indices = alpha > 1e-5\n",
    "        if np.any(sv_indices):\n",
    "            self.b = np.mean(y_binary[sv_indices] - np.dot(X[sv_indices], self.w))\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels for samples in X.\"\"\"\n",
    "        return np.where(np.dot(X, self.w) + self.b >= 0, 1, 0)\n",
    "\n",
    "    def get_parameters(self):\n",
    "      print(f'w: {self.w}')\n",
    "      print(f'b: {self.b}')\n",
    "\n",
    "    def decision_function(self, X):\n",
    "        \"\"\"Return distance of samples to the decision boundary.\"\"\"\n",
    "        return np.dot(X, self.w) + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "369fcf41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T00:54:37.657884Z",
     "iopub.status.busy": "2025-04-12T00:54:37.657322Z",
     "iopub.status.idle": "2025-04-12T00:54:37.663214Z",
     "shell.execute_reply": "2025-04-12T00:54:37.661939Z"
    },
    "id": "t3vSn1WbT-6m",
    "papermill": {
     "duration": 0.010876,
     "end_time": "2025-04-12T00:54:37.664945",
     "exception": false,
     "start_time": "2025-04-12T00:54:37.654069",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500, 5000, 5500, 6000, 6500, 7000, 7500, 8000, 8500, 9000, 9500]\n"
     ]
    }
   ],
   "source": [
    "a = [i for i in range(1000, 10000, 500)]\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9bbb748",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T00:54:37.670806Z",
     "iopub.status.busy": "2025-04-12T00:54:37.670435Z",
     "iopub.status.idle": "2025-04-12T00:54:37.683875Z",
     "shell.execute_reply": "2025-04-12T00:54:37.683020Z"
    },
    "id": "PMFloxHDPMgT",
    "papermill": {
     "duration": 0.018031,
     "end_time": "2025-04-12T00:54:37.685295",
     "exception": false,
     "start_time": "2025-04-12T00:54:37.667264",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calc_time_svm():\n",
    "  results = []\n",
    "  for i in a:\n",
    "    df = pd.read_csv(f\"https://media.githubusercontent.com/media/PTIT-Projects/ttcs-svm-spam-email/refs/heads/main/dataset/sampled_dataset{i}.csv\")\n",
    "\n",
    "    # viet thuong\n",
    "    df['text'] = df['text'].str.lower()\n",
    "\n",
    "    # xoa ky tu khong phai ASCII\n",
    "    df['text'] = df['text'].apply(lambda x: re.sub(r'[^\\x00-\\x7F]+', '', x) if isinstance(x, str) else x)\n",
    "\n",
    "    # xoa khoang trang\n",
    "    df['text'] = df['text'].apply(lambda x: re.sub(r'^\\s+|\\s+$', '', x).strip() if isinstance(x, str) else x)\n",
    "\n",
    "    # xoa html, xml\n",
    "    def remove_html_xml(text):\n",
    "        try:\n",
    "            soup = BeautifulSoup(text, 'html.parser')\n",
    "            return soup.get_text()\n",
    "        except:\n",
    "            return text\n",
    "\n",
    "    df['text'] = df['text'].apply(remove_html_xml)\n",
    "\n",
    "    # xoa ky tu dac biet\n",
    "    def remove_special_characters(word):\n",
    "        return word.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    df['text'] = df['text'].apply(remove_special_characters)\n",
    "\n",
    "    # xoa url\n",
    "    def remove_urls(text):\n",
    "        return re.sub(r'http\\S+|www\\S+|\\S+\\.(com|net|org|edu|gov|mil|int|info|biz|co)\\S+', '', text)\n",
    "\n",
    "    df['text'] = df['text'].apply(remove_urls)\n",
    "\n",
    "    # xoa dia chi email\n",
    "    def remove_emails(text):\n",
    "        return re.sub(r'\\S+@\\S+', '', text)\n",
    "\n",
    "    df['text'] = df['text'].apply(remove_emails)\n",
    "\n",
    "    # tach thanh cac tu\n",
    "    df['text'] = df['text'].apply(word_tokenize)\n",
    "\n",
    "    # xoa tu dung(tieng anh)\n",
    "    ENGLISH_STOP_WORDS = set(stopwords.words('english'))\n",
    "\n",
    "    def remove_stop_words(words):\n",
    "        return [word for word in words if word not in ENGLISH_STOP_WORDS]\n",
    "\n",
    "    df['text'] = df['text'].apply(remove_stop_words)\n",
    "\n",
    "    # cat goc tu\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    def stem_words(words):\n",
    "        return [stemmer.stem(word) for word in words]\n",
    "\n",
    "    df['text'] = df['text'].apply(stem_words)\n",
    "\n",
    "    # noi cac tu thanh chuoi\n",
    "    df['text'] = df['text'].apply(' '.join)\n",
    "\n",
    "    # dataset trainning voi test\n",
    "    X = df['text']\n",
    "    y = df['label']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "    # tfidf\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "    X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "    # # hashing_vectorizer\n",
    "    # hashing_vectorizer = HashingVectorizer(n_features=5000)\n",
    "    # X_train_hashed = hashing_vectorizer.fit_transform(X_train)\n",
    "    # X_test_hashed = hashing_vectorizer.transform(X_test)\n",
    "    X_train_dense = X_train_tfidf.toarray()\n",
    "    X_test_dense = X_test_tfidf.toarray()\n",
    "    result = []\n",
    "    svm_base = HardMarginSVM()\n",
    "    start_time = time.time()\n",
    "    svm_base.fit(X_train_dense, y_train)\n",
    "    end_time = time.time()\n",
    "    y_pred = svm_base.predict(X_test_dense)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    result.append({\n",
    "            'class_name': svm_base.__class__.__name__,\n",
    "            'time': end_time - start_time,\n",
    "            'accuracy_score': accuracy,\n",
    "            'f1_score': f1\n",
    "        })\n",
    "    results_df = pd.DataFrame(result)\n",
    "    print(results_df)\n",
    "    results_df.to_csv(f'hardmargin_svm_tfidf_{i}.csv', index=False)\n",
    "  return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d288ee9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T00:54:37.691172Z",
     "iopub.status.busy": "2025-04-12T00:54:37.690799Z"
    },
    "id": "IcQD3SHrYQFg",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": false,
     "start_time": "2025-04-12T00:54:37.687623",
     "status": "running"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      class_name        time  accuracy_score  f1_score\n",
      "0  HardMarginSVM  188.271099           0.945   0.94686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      class_name        time  accuracy_score  f1_score\n",
      "0  HardMarginSVM  483.728427            0.96  0.963415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      class_name         time  accuracy_score  f1_score\n",
      "0  HardMarginSVM  1025.564333            0.96  0.961353\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      class_name         time  accuracy_score  f1_score\n",
      "0  HardMarginSVM  1769.558942           0.976  0.976378\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      class_name         time  accuracy_score  f1_score\n",
      "0  HardMarginSVM  2834.332839        0.966667  0.966887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      class_name         time  accuracy_score  f1_score\n",
      "0  HardMarginSVM  4327.781785        0.967143  0.969935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      class_name         time  accuracy_score  f1_score\n",
      "0  HardMarginSVM  6007.968148         0.96375  0.967306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      class_name         time  accuracy_score  f1_score\n",
      "0  HardMarginSVM  8247.298119        0.962222  0.963907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      class_name          time  accuracy_score  f1_score\n",
      "0  HardMarginSVM  10635.539112           0.969  0.970837\n"
     ]
    }
   ],
   "source": [
    "calc_time_svm()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-12T00:54:27.572405",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}