import streamlit as st
import pandas as pd
import numpy as np
import re
import nltk
import string
import pickle
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from nltk.tokenize import word_tokenize
from bs4 import BeautifulSoup
from scipy.sparse import csr_matrix

@st.cache_resource
def download_nltk_resources():
    nltk.download('stopwords')
    nltk.download('punkt_tab')
    
download_nltk_resources()

class SVM:
    def __init__(self, lambda_param=1e-4, epoch=1000, batch_size=256, tol=1e-4, random_state=42):
        self.lambda_param = lambda_param
        self.epoch = epoch
        self.batch_size = batch_size
        self.tol = tol
        self.random_state = random_state
        self.is_trained = False

    def fit(self, X, y):
        if hasattr(X, "toarray"):
            X = csr_matrix(X)
        
        self.num_samples, self.num_features = X.shape

        y_unique = np.unique(y)
        if len(y_unique) != 2:
            raise ValueError("Ph√¢n lo·∫°i nh·ªã ph√¢n c·∫ßn 2 nh√£n")
        if set(y_unique) == {0, 1}:
            y = np.where(y == 0, -1, 1)
        
        self.w = np.zeros(self.num_features, dtype=np.float32)
        self.b = 0.0

        np.random.seed(self.random_state)
        t = 0
        previous_objective = float("inf")

        for ep in range(1, self.epoch + 1):
            indices = np.random.permutation(self.num_samples)
            for start in range(0, self.num_samples, self.batch_size):
                t += 1
                end = start + self.batch_size
                batch_idx = indices[start:end]
                X_batch = X[batch_idx]
                y_batch = y[batch_idx]
                
                eta = 1.0 / (self.lambda_param * t)
                margins = y_batch * (X_batch.dot(self.w) + self.b)
                mask = margins < 1
                self.w *= (1 - eta * self.lambda_param)
                if np.any(mask):
                    X_violate = X_batch[mask]
                    y_violate = y_batch[mask]
                    self.w += (eta / self.batch_size) * np.dot(y_violate, X_violate.toarray() if hasattr(X_violate, "toarray") else X_violate)
                    self.b += (eta / self.batch_size) * np.sum(y_violate)
                norm_w = np.linalg.norm(self.w)
                factor = min(1, (1.0 / np.sqrt(self.lambda_param)) / (norm_w))
                self.w *= factor

            decision = X.dot(self.w) + self.b
            hinge_losses = np.maximum(0, 1 - y * decision)
            objective = 0.5 * self.lambda_param * np.dot(self.w, self.w) + np.mean(hinge_losses)
            
            if ep % 10 == 0:
                print(f"Epoch {ep}, Gi√° tr·ªã h√†m m·ª•c ti√™u: {objective:.4f}")
            
            if abs(previous_objective - objective) < self.tol:
                print(f"D·ª´ng s·ªõm t·∫°i epoch {ep}, gi√° tr·ªã h√†m m·ª•c ti√™u thay ƒë·ªïi: {abs(previous_objective - objective):.6f}")
                break
            previous_objective = objective

        self.is_trained = True
        return self

    def predict(self, X):
        if not self.is_trained:
            raise Exception("M√¥ h√¨nh ch∆∞a ƒë∆∞·ª£c hu·∫•n lu·ªµen")
            
        if hasattr(X, "toarray"):
            X = csr_matrix(X)
            
        decision = X.dot(self.w) + self.b
        return np.where(decision >= 0, 1, 0)

@st.cache_resource
def load_model():
    try:
        with open('linear_svm.pkl', 'rb') as model_file:
            model = pickle.load(model_file)
        with open('vectorizer.pkl', 'rb') as vectorizer_file:
            vectorizer = pickle.load(vectorizer_file)
        return model, vectorizer
    except FileNotFoundError:
        st.error("Kh√¥ng t√¨m th·∫•y file model/vectorizer")
        return None, None

model, vectorizer = load_model()

def remove_html_xml(text):
    try:
        soup = BeautifulSoup(text, 'html.parser')
        return soup.get_text()
    except:
        return text

def remove_special_characters(word):
    return word.translate(str.maketrans('', '', string.punctuation))

def remove_urls(text):
    return re.sub(r'http\S+|www\S+|\S+\.(com|net|org|edu|gov|mil|int|info|biz|co)\S+', '', text)

def remove_emails(text):
    return re.sub(r'\S+@\S+', '', text)

def preprocess_text(text):
    text = text.lower()
    text = re.sub(r'[^\x00-\x7F]+', '', text) if isinstance(text, str) else text
    text = re.sub(r'^\s+|\s+$', '', text).strip() if isinstance(text, str) else text
    text = remove_html_xml(text)
    text = remove_special_characters(text)
    text = remove_urls(text)
    text = remove_emails(text)
    tokens = word_tokenize(text)
    ENGLISH_STOP_WORDS = set(stopwords.words('english'))
    tokens = [word for word in tokens if word not in ENGLISH_STOP_WORDS]
    stemmer = PorterStemmer()
    tokens = [stemmer.stem(word) for word in tokens]
    return ' '.join(tokens)


st.title("Demo ph√¢n lo·∫°i email ti·∫øng Anh spam ")


user_input = st.text_area("Nh·∫≠p n·ªôi dung email:", height=200)

if st.button("Ki·ªÉm tra"):
    if not user_input:
        st.warning("H√£y nh·∫≠p n·ªôi dung ƒë·ªÉ ph√¢n t√≠ch")
    elif model is None or vectorizer is None:
        st.error("Model/vectorizer kh√¥ng load ƒë∆∞·ª£c")
    else:
        with st.spinner("ƒêang ph√¢n t√≠ch..."):
            preprocessed_text = preprocess_text(user_input)
            if hasattr(vectorizer, 'transform'):
                features = vectorizer.transform([preprocessed_text])
            else:
                st.error("Kh√¥ng t√¨m th·∫•y vectorizer")
            prediction = model.predict(features)
            if prediction[0] == 1:
                st.error("üö® Email c√≥ kh·∫£ nƒÉng l√† SPAM")
            else:
                st.success("‚úÖEmail c√≥ kh·∫£ nƒÉng kh√¥ng ph·∫£i l√† SPAM ")
            
            st.write("### VƒÉn b·∫£n sau khi ƒë∆∞·ª£c ti·ªÅn x·ª≠ l√Ω:")
            st.write(preprocessed_text)